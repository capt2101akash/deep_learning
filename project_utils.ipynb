{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_h5():\n",
    "     data_dir = './dataset'\n",
    "     total_np_x = list()\n",
    "     total_np_y = list()\n",
    "     total_np_log_x = list()\n",
    "     save_path = f'./dataset/full_data.h5'\n",
    "     import matplotlib.pyplot as plt\n",
    "     import time\n",
    "     from PIL import Image\n",
    "     for dir in os.listdir(data_dir):\n",
    "          count = 0\n",
    "          for file in os.walk(os.path.join(data_dir, dir)):\n",
    "               if len(file[2]) > 0:\n",
    "                    print(file[0], len(file[2]))\n",
    "                    for img_file in file[2]:\n",
    "                         img_path = os.path.join(file[0], img_file)\n",
    "                         img_f = Image.open(img_path)\n",
    "                         orig_img = np.asarray(img_f)\n",
    "                         resized = Image.fromarray(orig_img).resize(size=(80, 80))\n",
    "                         resized_array = np.asarray(resized)\n",
    "                         resized_squeezed = resized_array.reshape(80*80*3, 1)\n",
    "                         resized_log = Image.fromarray(resized_array).resize(size = (2, 2))\n",
    "                         resized_log_squeezed = np.asarray(resized_log)\n",
    "                         resized_log_squeezed = resized_log_squeezed.reshape(2*2*3, 1)\n",
    "                         # print(resized_squeezed.shape)\n",
    "                         total_np_x.append(resized_squeezed)\n",
    "                         total_np_log_x.append(resized_log_squeezed)\n",
    "                         if 'cat' in img_file:\n",
    "                              total_np_y.append(0)\n",
    "                         else:\n",
    "                              total_np_y.append(1)\n",
    "               \n",
    "                         count += 1\n",
    "               if count % 1000:\n",
    "                    print(count)\n",
    "               # break\n",
    "          # break\n",
    "\n",
    "     total_np_x = np.squeeze(np.asarray(total_np_x), axis = 2)\n",
    "     total_np_y = np.squeeze(np.asarray(total_np_y))\n",
    "     print(np.asarray(total_np_log_x).shape)\n",
    "     total_np_log_x = np.squeeze(np.asarray(total_np_log_x), axis = 2)\n",
    "     hf = h5py.File(save_path, 'a') \n",
    "     grp_x = hf.create_dataset('data_x', data = total_np_x)\n",
    "     grp_y = hf.create_dataset('data_y', data = total_np_y)\n",
    "     grp_x_log = hf.create_dataset('data_x_log', data = total_np_log_x)\n",
    "\n",
    "     hf.close()\n",
    "     \n",
    "     # print('hdf5 file size: %d bytes'%os.path.getsize(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    total_dataset = h5py.File('./dataset/full_data3.h5', \"r\")\n",
    "\n",
    "    # Test Set\n",
    "    test_x_d = total_dataset[\"data_x\"][1000:2000]\n",
    "    test_y_d = total_dataset[\"data_y\"][1000:2000]\n",
    "\n",
    "    test_x_c = total_dataset[\"data_x\"][:1000]\n",
    "    test_y_c = total_dataset[\"data_y\"][:1000]\n",
    "\n",
    "    test_x = np.vstack((test_x_c, test_x_d))\n",
    "    test_y = np.append(test_y_c, test_y_d)\n",
    "\n",
    "\n",
    "    # Training Set\n",
    "    train_x_d = total_dataset[\"data_x\"][7000:10000]\n",
    "    train_y_d = total_dataset[\"data_y\"][7000:10000]\n",
    "\n",
    "    train_x_c = total_dataset[\"data_x\"][2000:5000]\n",
    "    train_y_c = total_dataset[\"data_y\"][2000:5000]\n",
    "\n",
    "    train_y = np.append(train_y_c, train_y_d)\n",
    "    train_x = np.vstack((train_x_c, train_x_d))\n",
    "\n",
    "\n",
    "    train_x_d_log = total_dataset[\"data_x_log\"][7000:10000]\n",
    "\n",
    "    train_x_c_log = total_dataset[\"data_x_log\"][2000:5000]\n",
    "\n",
    "    train_x_log = np.vstack((train_x_c_log, train_x_d_log))\n",
    "    # Validation Set\n",
    "\n",
    "    valid_x_d = total_dataset[\"data_x\"][6000:7000]\n",
    "    valid_y_d = total_dataset[\"data_y\"][6000:7000]\n",
    "\n",
    "    valid_x_c = total_dataset[\"data_x\"][5000:6000]\n",
    "    valid_y_c = total_dataset[\"data_y\"][5000:6000]\n",
    "\n",
    "    valid_y = np.append(valid_y_c, valid_y_d)\n",
    "    valid_x = np.vstack((valid_x_c, valid_x_d))\n",
    "\n",
    "    # assert train_x.shape == (6000, 10800, 1)\n",
    "    # assert test_x.shape == (2000, 10800, 1)\n",
    "    # assert valid_x.shape == (2000, 10800, 1)\n",
    "    \n",
    "    return train_x, train_x_log, train_y, test_x, test_y, valid_x, valid_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset\\test_set\\cats 1000\n",
      "./dataset\\test_set\\dogs 1000\n",
      "./dataset\\training_set\\cats 4000\n",
      "./dataset\\training_set\\dogs 4000\n",
      "(10000, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "convert_to_h5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
